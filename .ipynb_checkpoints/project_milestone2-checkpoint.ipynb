{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project milestone 2, CS-401 : Applied data analysis, group : TheAvengers\n",
    "\n",
    "\n",
    "This Notebook can be divided in two parts. First, we handled our initial CMU dataset by enriching with other movies datasets and preprocess it. Then, we started with a first analysis...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a> \n",
    " # Table of Contents  \n",
    "1. [Import libraries and python files](#1)     \n",
    "1. [Load the CMU dataset and additional datasets](#2)    \n",
    "1. [Let's start the analysis](#4)     \n",
    "1. [First Model](#8)     \n",
    "    1. [Evaluation Metrics for Training set](#9)     \n",
    "    1. [Evaluation Metrics for Validation set](#10)     \n",
    "    1. [First Submission](#11) \n",
    "1. [Selecting Models](#12)       \n",
    "    1. [Helper Functions to Try New Models](#13)      \n",
    "    1. [Split to the Small Data for Evaluating Models Fast](#14)     \n",
    "    1. [ML Models](#15)         \n",
    "        1. [XGBoost](#16)             \n",
    "            1. [Training](#17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    "## 1. Import libraries and python files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import requests\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from data_loader import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    "## 2. Load the CMU dataset and additional datasets\n",
    "\n",
    "Lets first download our datasets and  extract the former countriers, the genres and the languages from each movie and create new columns feature that will be able to access more easily on the mentionned datas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/MovieSummaries/movie.metadata.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#loading the datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataFrame_movie \u001b[38;5;241m=\u001b[39m \u001b[43mload_cmu_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dataFrame_character \u001b[38;5;241m=\u001b[39m load_cmu_characters()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#dataFrame_kaggle_movie = load_kaggle_movies()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#extracting the aforementioned features\u001b[39;00m\n",
      "File \u001b[0;32m~/ada-2023-project-theavengers2023/data_loader.py:33\u001b[0m, in \u001b[0;36mload_cmu_movies\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_cmu_movies\u001b[39m():\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#load the dataset\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     df_movie \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMOVIE_DATASET\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     df_movie\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWikipedia ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreebase ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelease date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue\u001b[39m\u001b[38;5;124m'\u001b[39m, \\\n\u001b[1;32m     35\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRun Time\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLanguage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountries\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenres\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m#convert the 'Release date' column of movie dataset to YYYY-MM-DD format with 3 new columns : 'Year', 'Day'\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.9/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.9/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.9/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/MovieSummaries/movie.metadata.tsv'"
     ]
    }
   ],
   "source": [
    "#loading the datasets\n",
    "dataFrame_movie = load_cmu_movies()\n",
    "dataFrame_character = load_cmu_characters()\n",
    "#dataFrame_kaggle_movie = load_kaggle_movies()\n",
    "\n",
    "#extracting the aforementioned features\n",
    "dataFrame_movie['Country_List'] = dataFrame_movie['Countries'].apply(extract_features_names)\n",
    "dataFrame_movie['Genre_List'] = dataFrame_movie['Genres'].apply(extract_features_names)\n",
    "dataFrame_movie['Language_List'] = dataFrame_movie['Language'].apply(extract_features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to add additional datasets in order to complete our datas. However, trying to merge those datasets based on their Movies names doesn't work as there are movies that have the same Name but are in fact different. Therefore, it is complicated to differentiate movies based on their names. To overcome this issue, we identified each movie by its Freebase ID or its IMDB ID (depending on the dataset). To do so, we first need to retreive the IMDB ID (i.e. tconst column) based on the movie's Freebase ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_id_translation = get_wikidata_id_translations()\n",
    "#display(df_id_translation)\n",
    "\n",
    "#save the dataframe to a JSON file\n",
    "#df_id_translation.to_json('id-translation.wikidata.json', orient='records', lines=True)\n",
    "\n",
    "#replace 'id-translation.wikidata.json' with the path to your JSON file\n",
    "file_path = 'id-translation.wikidata.json'\n",
    "\n",
    "#load the JSON file into a DataFrame\n",
    "df_id_translation = pd.read_json(file_path, orient='records', lines=True)\n",
    "\n",
    "#display the loaded DataFrame\n",
    "display(df_id_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can drop Nan and duplicates values and merge it with the CMU movies dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id_translation = df_id_translation.dropna(subset=['Freebase ID'])\n",
    "df_id_translation.drop_duplicates(subset=['Freebase ID'], keep='first', inplace=True)\n",
    "\n",
    "dataFrame_movie = pd.merge(dataFrame_movie, df_id_translation, on='Freebase ID', how='left')\n",
    "\n",
    "print(dataFrame_movie.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute percentage of missing values for df_mvoei\n",
    "percentage_missing_values_year = (dataFrame_movie['Year'].isna().sum()/len(dataFrame_movie['Year']))*100\n",
    "print(f\"The percentage of missing values 'Year' release date is {format(percentage_missing_values_year, '.3f')}%.\")\n",
    "\n",
    "percentage_missing_values_month = (dataFrame_movie['Month'].isna().sum()/len(dataFrame_movie['Month']))*100\n",
    "print(f\"The percentage of missing values 'Month' release date (and therefore 'Day') is {format(percentage_missing_values_month, '.3f')}%.\")\n",
    "\n",
    "percentage_missing_values_revenues = (dataFrame_movie['Revenue'].isna().sum()/len(dataFrame_movie['Revenue']))*100\n",
    "print(f\"The percentage of missing values 'Box office' is {format(percentage_missing_values_revenues, '.3f')}%.\")\n",
    "\n",
    "#percentage_missing_values_runtime = (dataFrame_movie['Runtime'].isna().sum()/len(dataFrame_movie['Runtime']))*100\n",
    "#print(f\"The percentage of missing values 'Runtime' is {format(percentage_missing_values_runtime, '.3f')}%.\")\n",
    "\n",
    "percentage_missing_values_tconst = (dataFrame_movie['tconst'].isna().sum()/len(dataFrame_movie['tconst']))*100\n",
    "print(f\"The percentage of missing values 'tconst' is {format(percentage_missing_values_tconst, '.3f')}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load addtionnal datasets and merge what we need "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load imdb dataset (from kaggle)\n",
    "dataFrame_imdb_movie = load_movie_imdb_kaggle()\n",
    "dataFrame_imdb_rating = load_rating_imdb_kaggle() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge movies with rating \n",
    "df_movie_rating = pd.merge(dataFrame_imdb_movie, dataFrame_imdb_rating, on='tconst', how='inner')\n",
    "df_movie_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge CMU dataset with IMDB dataset\n",
    "dataFrame_movie = pd.merge(dataFrame_movie, df_movie_rating[['tconst', 'averageRating', 'numVotes']], on=['tconst'], how='left')\n",
    "print(dataFrame_movie.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load kaggle movies dataset\n",
    "dataFrame_kaggle_movie = load_kaggle_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the two datasets\n",
    "dataFrame_movie = dataFrame_movie.merge(dataFrame_kaggle_movie[['Name', 'tconst', 'Revenue']], on=['Name', 'tconst'], how='left', suffixes=('', '_df2'))\n",
    "\n",
    "#use 'combine_first' to fill in the 'Revenue' values from df_kaggle_movie where they are NaN in df_movie\n",
    "dataFrame_movie['Revenue'] = dataFrame_movie['Revenue'].combine_first(dataFrame_movie['Revenue_df2'])\n",
    "\n",
    "#drop the 'Revenue_df2' column\n",
    "dataFrame_movie.drop('Revenue_df2', axis=1, inplace=True)\n",
    "\n",
    "#drop duplicates\n",
    "dataFrame_movie = dataFrame_movie.drop_duplicates(subset='Wikipedia ID', keep='first')\n",
    "print(dataFrame_movie.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_missing_values_revenues = (dataFrame_movie['Revenue'].isna().sum()/len(dataFrame_movie['Revenue']))*100\n",
    "print(f\"The percentage of missing values 'Revenue' is {format(percentage_missing_values_revenues, '.3f')}%.\")\n",
    "\n",
    "percentage_missing_values_rating = (dataFrame_movie['averageRating'].isna().sum()/len(dataFrame_movie['averageRating']))*100\n",
    "print(f\"The percentage of missing values 'averageRating' is {format(percentage_missing_values_rating, '.3f')}%.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to regroup all actors that have played on the same movie and add this new features to our dataframe. We will fix a threshold to keep only actors that have played a certain number of movies (in order to make our analysis feasible due to the large size of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame_movie.to_csv('start_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataFrame_movie from the start_dataset file and the dataFrame_character \n",
    "#dataFrame_movie = pd.read_csv('start_dataset.csv')\n",
    "#dataFrame_character = load_cmu_characters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of different movies each actor played in\n",
    "actor_movie_count = dataFrame_character.groupby('Actor Name')['Freebase ID'].nunique().reset_index()\n",
    "\n",
    "#rename the columns to match your requirements\n",
    "actor_movie_count.columns = ['Actor Name', 'Number of Movies']\n",
    "\n",
    "actor_movie_count_sorted = actor_movie_count.sort_values('Number of Movies', ascending=False).reset_index(drop=True)\n",
    "\n",
    "#fix a threshold for the minimum number of movies an actor must have played in to keep the row, we fix it to 0 for the moment \n",
    "threshold = 0 \n",
    "\n",
    "#filter dataFrame_character to only include actors who have played in more than the threshold number of movies\n",
    "# First, we create a list of actors who meet the threshold criteria\n",
    "actors_above_threshold = actor_movie_count_sorted[actor_movie_count_sorted['Number of Movies'] >= threshold]['Actor Name']\n",
    "\n",
    "#filter the original DataFrame\n",
    "dataFrame_character_filtered = dataFrame_character[dataFrame_character['Actor Name'].isin(actors_above_threshold)]\n",
    "\n",
    "display(dataFrame_character_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group dataFrame_character by Freebase ID and list the characters \n",
    "dataFrame_actor = dataFrame_character_filtered.groupby('Freebase ID')['Actor Name'].apply(list).reset_index()\n",
    "\n",
    "dataFrame_actor.columns = ['Freebase ID', 'List of known actors']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets merge this new dataframe with the DataFrame_movie\n",
    "dataFrame_movie = dataFrame_movie.merge(dataFrame_actor, on='Freebase ID', how='left')\n",
    "\n",
    "#create a new column that count the number of known actors per movie based on the list of actors\n",
    "dataFrame_movie['Number of known actors'] = dataFrame_movie['List of known actors'].apply(count_known_actors)\n",
    "\n",
    "print(dataFrame_movie.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can take a look on the percentage of movies for which the list of actors is not available and then extract that will be useful for the analysis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first make a copy of the dataset \n",
    "test = dataFrame_movie.copy()\n",
    "\n",
    "#we define a function on the data_loader file that allows us to check if a list is empty or not\n",
    "test['Non-Empty actors'] = test['List of known actors'].apply(is_nonempty_list)\n",
    "\n",
    "#compute the percentage of movies with empty or NaN actors lists\n",
    "percentage_with_nonempty_actors = (test[test['Non-Empty actors']].shape[0] / test.shape[0]) * 100\n",
    "\n",
    "\n",
    "print(f\"The percentage of missing values 'List of known actors' is {format(100 - percentage_with_nonempty_actors, '.3f')}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    "## 3. Let's start with the analysis\n",
    "\n",
    "Let's start by removing Nan values from the revenues as well as revenues less equal or equal to 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame_movie = dataFrame_movie.dropna(subset=['Revenue'])\n",
    "dataFrame_movie = dataFrame_movie[dataFrame_movie['Revenue'] >= 1]\n",
    "\n",
    "#sanity check lets compute the percentage of missing values for the revenues \n",
    "percentage_missing_values_revenues = (dataFrame_movie['Revenue'].isna().sum()/len(dataFrame_movie['Revenue']))*100\n",
    "print(f\"The percentage of missing values 'Revenue' is {format(percentage_missing_values_revenues, '.3f')}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by tacking a look on the distributions of the following features : countries, genres and language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the percentage of movies per countries, genres and languages with including only the top 5 fives categories\n",
    "top5_countries = (dataFrame_movie['Country_List'].apply(pd.Series).stack().value_counts()[:5])/dataFrame_movie.shape[0]\n",
    "top5_languages = (dataFrame_movie['Language_List'].apply(pd.Series).stack().value_counts()[:5])/dataFrame_movie.shape[0]\n",
    "top5_genres = (dataFrame_movie['Genre_List'].apply(pd.Series).stack().value_counts()[:5])/dataFrame_movie.shape[0]\n",
    "\n",
    "# remove \"Language\" in the label because it is not useful \n",
    "labels_language = [label.replace('Language', '') for label in top5_languages.index]\n",
    "\n",
    "#plot the pie charts\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Percentage of movies per top 5 countries, languages and genres')\n",
    "axes[0].pie(top5_countries, labels=top5_countries.index, autopct='%1.1f%%', shadow=False, startangle=90)\n",
    "axes[0].set_title('Countries')\n",
    "axes[1].pie(top5_languages, labels=labels_language, autopct='%1.1f%%', shadow=False, startangle=90)\n",
    "axes[1].set_title('Languages')\n",
    "axes[2].pie(top5_genres, labels=top5_genres.index, autopct='%1.1f%%', shadow=False, startangle=90)\n",
    "axes[2].set_title('Genres')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD COMMENTS !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Revenues\n",
    "#scales are very high on the x axis so we are gonna plot log(x) against y\n",
    "dataFrame_movie['Revenue'].plot(kind='hist', logy=True)\n",
    "plt.xlabel('Revenue')\n",
    "plt.ylabel('count [log]')\n",
    "plt.title('Histogram of Revenues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame_movie['Revenue'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution of revenues during time \n",
    "\n",
    "df = dataFrame_movie[['Year','Revenue']].groupby(['Year']).mean()\n",
    "df['std'] = dataFrame_movie[['Year','Revenue']].groupby(['Year']).std()\n",
    "df['Year']=df.index\n",
    "#drop first value \n",
    "df = df[df['Year'] > 1900]\n",
    "\n",
    "#plot \n",
    "df.plot(x = 'Year', y='Revenue', logy= True)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Revenue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Average Ratings\n",
    "sns.histplot(data=dataFrame_movie, x='averageRating', bins=40, kde = True)\n",
    "plt.title('Histogram of Average Rating')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive data for the Average Rating\n",
    "dataFrame_movie['averageRating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution of ratings during time \n",
    "df2 = dataFrame_movie[['Year','averageRating']].groupby(['Year']).mean()\n",
    "df2['std'] = dataFrame_movie[['Year','averageRating']].groupby(['Year']).std()\n",
    "df2['Year']=df2.index\n",
    "df2 = df2[df2['Year'] > 1900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the Evolution of average rating\n",
    "df2.plot(x = 'Year', y='averageRating')\n",
    "plt.xlabel('Year [-]')\n",
    "plt.ylabel('Average Rating [-]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter plot between the average Rating and the Revenue \n",
    "sns.jointplot(x=dataFrame_movie['averageRating'], y= np.log(dataFrame_movie['Revenue']), kind=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation when we group by year\n",
    "df2['Revenue']= dataFrame_movie[['Year','Revenue']].groupby(['Year']).mean()['Revenue']\n",
    "df2['Revenue'] = np.log(df2['Revenue'])\n",
    "df2.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace inf with NaN\n",
    "df2 = df2.dropna()  # Drop rows with NaN or inf\n",
    "stats.pearsonr(df2['averageRating'],df2['Revenue'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation when we group by Country\n",
    "copy_data = dataFrame_movie.copy(deep=True)\n",
    "copy_data['Country_List'] = copy_data['Country_List'].apply(tuple)\n",
    "copy_data['Revenue'] = np.log(copy_data['Revenue'])\n",
    "df3 = copy_data[['Revenue','Country_List']].groupby(['Country_List']).mean()\n",
    "df3['averageRating']= copy_data[['averageRating','Country_List']].groupby(['Country_List']).mean()\n",
    "df3.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace inf with NaN\n",
    "df3 = df3.dropna()  # Drop rows with NaN or inf\n",
    "stats.pearsonr(df3['averageRating'],df3['Revenue'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation when we group by Language\n",
    "copy_data['Language_List'] = copy_data['Language_List'].apply(tuple)\n",
    "df3 = copy_data[['Revenue','Language_List']].groupby(['Language_List']).mean()\n",
    "df3['averageRating']= copy_data[['averageRating','Language_List']].groupby(['Language_List']).mean()\n",
    "df3.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace inf with NaN\n",
    "df3 = df3.dropna()  # Drop rows with NaN or inf\n",
    "stats.pearsonr(df3['averageRating'],df3['Revenue']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are counting the number of actor per film (actor = character) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = dataFrame_movie.merge(dataFrame_character, on = 'Wikipedia ID', how = 'left') # POPULATE EACH MOVIE WITH ACTOR \n",
    "counter = df_count.groupby('Wikipedia ID')['Actor Name'].nunique() # We count the number of actor per movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.hist(counter, bins =max(counter))\n",
    "\n",
    "plt.title('Frequency of the Number of actor per movie')\n",
    "plt.xlabel('Number of actor per movie')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the idea is to create a matrix. Each row of the matrix represent a differente movie. Every Column represents a different actor. The matrix returns 1 if the actor i plays in the movie j and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary matrix where rows represent movies, columns represent actors\n",
    "binary_matrix = df_count.pivot_table(index='Name', columns='Actor Name', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Reset the index to have movie name as a column\n",
    "binary_matrix = binary_matrix.reset_index()\n",
    "\n",
    "# Fill NaN values with 0 (if necessary)\n",
    "binary_matrix = binary_matrix.fillna(0)\n",
    "\n",
    "# Rename the columns with a prefix for clarity\n",
    "binary_matrix.columns = ['Name'] + [ str(col) for col in binary_matrix.columns[1:]]\n",
    "\n",
    "# Set 'movie name' as the index \n",
    "binary_matrix.set_index('Name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to much actor right now, therefore we have to get rid of many of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this matrix, we will remove the actors that appears in less than 35 movies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_sum=binary_matrix.sum()\n",
    "actors_to_drop=actor_sum[actor_sum<35].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_binary_matrix=binary_matrix.drop(actors_to_drop,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_binary_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in filtered_binary_matrix.columns.tolist():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with this filter, we still have a high number of actor (113). This is still a high number and if we want to test every pair of actor as a independant variable, we need (113*112)/2 beta. We still have the possibility to filter more using maybe more qualitative aspect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important aspect that we have to care about is the interaction between actors. We build now the matrix of interraction between actors with more than 25 films. (Why 25? because it is better to have a bigger number of actor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_to_drop=actor_sum[actor_sum<25].index\n",
    "\n",
    "filtered_binary_matrix=binary_matrix.drop(actors_to_drop,axis=1)\n",
    "\n",
    "filtered_binary_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each column of the filtered_binary_matrix, we multiply by every other column and them sum the new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_matrix_column = filtered_binary_matrix.columns\n",
    "\n",
    "\n",
    "matrix = np.zeros([len(binary_matrix_column),len(binary_matrix_column)])\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "# Iteration in every row and column to calculate the interaction\n",
    "for col in binary_matrix_column:\n",
    "    result = []\n",
    "    j = 0\n",
    "    for col2 in binary_matrix_column:\n",
    "        #since the column are dummies, we can just multiply\n",
    "        matrix[i,j]= (filtered_binary_matrix[col]* filtered_binary_matrix[col2]).sum()\n",
    "        j+= 1\n",
    "    i+=1\n",
    "\n",
    "# We put the result in a dataframe\n",
    "pairing_df = pd.DataFrame(matrix, columns=binary_matrix_column, index=binary_matrix_column)\n",
    "\n",
    "# We save the result in a csv file\n",
    "pairing_df.to_csv('interaction_matrix_actors.csv', index=False) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our matrix of interaction. We will sort a list of actor that interract the most with his pear. We set the diagonal of the matrix equal to 0 and count the number of interaction of each actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_interaction = matrix - matrix*np.eye(len(binary_matrix_column))\n",
    "pure_interaction = pd.DataFrame(matrix_interaction, columns=binary_matrix_column, index=binary_matrix_column)\n",
    "\n",
    "pure_interaction = pure_interaction.sum(axis =1)\n",
    "\n",
    "pure_interaction.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of actors who collaborate most frequently with their peers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Idea : We should maybe draw a map of the connection between actor to get rid of interration between actor that have never worked together. We should maybe add more women (even if their number of film is lower). We could maybe see an intresting trend\n",
    "We could also use other types of variable such as the age difference between the actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try to create a graph network\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#create a mapping from actor names to numbers (for better visualization)\n",
    "actor_to_number = {actor: num for num, actor in enumerate(pairing_df.columns)}\n",
    "\n",
    "#create a graph\n",
    "G = nx.from_pandas_adjacency(pairing_df)\n",
    "G = nx.relabel_nodes(G, actor_to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets fix a treshold to remove the edges with a low interaction in order to have a better visualization\n",
    "threshold = 50 #we can change this value to see the difference  \n",
    "for u, v, d in list(G.edges(data=True)):\n",
    "    if d['weight'] <= threshold:\n",
    "        G.remove_edge(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos = nx.spring_layout(G, scale=2)  # positions for all nodes\n",
    "#weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "edge_widths = [d['weight'] for u, v, d in G.edges(data=True)]  \n",
    "\n",
    "#nx.draw(G, pos, edges=G.edges(), width=weights, with_labels=True, node_size=50, font_size=8)\n",
    "nx.draw(G, pos, width=edge_widths, with_labels=True, node_size=70, font_size=12, alpha=0.7)\n",
    "\n",
    "#adjust the label position\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=5)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
